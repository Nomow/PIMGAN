{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.stylegan2 import * \n",
    "from datasets.UVGan import UVGanDataset1\n",
    "from metrics.loss import FaceIDLoss\n",
    "import json\n",
    "from metrics import metric_main\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\"\n",
    "style_mixing_prob = 0\n",
    "batch_size = 1\n",
    "num_workers = 4\n",
    "epochs = 500\n",
    "data_path = \"/src/data/raw_data/ffhq_with_mask\"\n",
    "d_reg_interval = 16\n",
    "g_reg_interval = 4\n",
    "ema_kimg = 10\n",
    "ada_target = 0.6  \n",
    "ada_interval = 4     \n",
    "ada_kimg = 300\n",
    "save_interval_kimgs = 35\n",
    "stats_collector = training_stats.Collector(regex='.*')\n",
    "stats_metrics = dict()\n",
    "run_dir = \"/src/current-approach/new/snapshots-stylegan-detected-mask11111222222\"\n",
    "os.makedirs(run_dir, exist_ok=True)\n",
    "stats_jsonl = open(os.path.join(run_dir, 'stats.jsonl'), 'wt')\n",
    "stats_interval_kimgs = 1\n",
    "metric_interval_kimgs = 10\n",
    "model_save_interval_kimgs = 30\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256 aaaaa\n",
      "Stylegan generator trained model found. load /src/current-approach/synthethic-face-generation-and-manipulation/snapshots/snapshot/network-snapshot-11.pkl\n"
     ]
    }
   ],
   "source": [
    "cfg_dict = {\n",
    "    \"pretrained_model\" : \"/src/current-approach/synthethic-face-generation-and-manipulation/snapshots/snapshot/network-snapshot-11.pkl\",\n",
    "    \"img_size\" : 256,\n",
    "    \"cond_dim\" : 0,\n",
    "    \"channels\" : 3,\n",
    "\n",
    "    \"G_kwargs\": {\n",
    "        \"z_dim\": 50,\n",
    "        \"w_dim\": 512,\n",
    "        \"mapping_kwargs\": {\n",
    "          \"num_layers\": 8\n",
    "        },\n",
    "    \"synthesis_kwargs\": {\n",
    "        \"channel_base\": 16384,\n",
    "        \"channel_max\": 512,\n",
    "        \"num_fp16_res\": 4,\n",
    "        \"conv_clamp\": 256\n",
    "      }\n",
    "    }\n",
    "}\n",
    "\n",
    "cfg = DefaultMunch.fromDict(cfg_dict)\n",
    "z_dim = cfg.G_kwargs.z_dim\n",
    "c_dim = cfg.cond_dim\n",
    "G, G_ema = create_generator(cfg, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stylegan discriminator trained model not found found, creating new model \n"
     ]
    }
   ],
   "source": [
    "cfg_dict = {\n",
    "    \"pretrained_model\" : \"\",\n",
    "    \"img_size\" : 256,\n",
    "    \"cond_dim\" : 0,\n",
    "    \"channels\" : 3,\n",
    "    \"D_kwargs\": {\n",
    "      \"block_kwargs\": {},\n",
    "      \"mapping_kwargs\": {},\n",
    "      \"epilogue_kwargs\": {\n",
    "        \"mbstd_group_size\": 8\n",
    "      },\n",
    "      \"channel_base\": 16384,\n",
    "      \"channel_max\": 512,\n",
    "      \"num_fp16_res\": 4,\n",
    "      \"conv_clamp\": 256\n",
    "    },\n",
    "}\n",
    "\n",
    "cfg = DefaultMunch.fromDict(cfg_dict)\n",
    "D = create_discrimnator(cfg, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_dict = {\n",
    "\"p\" : 0,\n",
    "\"augment_kwargs\": {\n",
    "    \"xflip\": 1,\n",
    "    \"rotate90\": 1,\n",
    "    \"xint\": 1,\n",
    "    \"scale\": 1,\n",
    "    \"rotate\": 1,\n",
    "    \"aniso\": 1,\n",
    "    \"xfrac\": 1,\n",
    "    \"brightness\": 1,\n",
    "    \"contrast\": 1,\n",
    "    \"lumaflip\": 1,\n",
    "    \"hue\": 1,\n",
    "    \"saturation\": 1\n",
    "  },\n",
    "}\n",
    "cfg = DefaultMunch.fromDict(cfg_dict)\n",
    "\n",
    "augment_pipe = create_augment_pipe(cfg, device)\n",
    "ada_stats = training_stats.Collector(regex='Loss/signs/real')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "cfg_dict = {   \n",
    "    \"snapshot_nimg\": 30000,\n",
    "    \"ema_kimg\": 10,\n",
    "    \"G_opt_kwargs\": {\n",
    "      \"lr\": 0.001,\n",
    "      \"betas\": [\n",
    "        0,\n",
    "        0.99\n",
    "      ],\n",
    "      \"eps\": 1e-08\n",
    "    },\n",
    "    \"D_opt_kwargs\": {\n",
    "      \"lr\": 0.001,\n",
    "      \"betas\": [\n",
    "        0,\n",
    "        0.99\n",
    "      ],\n",
    "      \"eps\": 1e-08\n",
    "    },\n",
    "}\n",
    "\n",
    "cfg = DefaultMunch.fromDict(cfg_dict)\n",
    "\n",
    "optimizer_G = torch.optim.Adam(G.parameters(), **cfg.G_opt_kwargs)\n",
    "optimizer_D = torch.optim.Adam(D.parameters(), **cfg.D_opt_kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_reg_loss = PathLengthLoss(G, device)\n",
    "r1_reg_loss = R1Regularization(D, device, augment_pipe=augment_pipe)\n",
    "discrim_loss = DiscriminatorLoss(D, device, augment_pipe_real=augment_pipe)\n",
    "# face_id_loss = FaceIDLoss(\"r50\", \"/src/current-approach/synthethic-face-generation-and-manipulation/third_part/backbone.pth\", 256, 112).to(device)\n",
    "mse_loss = torch.nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = UVGanDataset1(data_path)\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up PyTorch plugin \"bias_act_plugin\"... Done.\n",
      "Setting up PyTorch plugin \"upfirdn2d_plugin\"... Done.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cannot reshape tensor of 0 elements into shape [0, 1, -1, 1, 1] because the unspecified dimension size -1 can be any value and is ambiguous",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1321328/464017849.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_idx\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mg_reg_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0mg_reg_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0muv_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mws\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_G\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstyle_mixing_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/src/current-approach/new/utils/stylegan2.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, gen_z, gen_c)\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Gpl_forward'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_z\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpl_batch_shrink\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m             \u001b[0mgen_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_ws\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_G\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_z\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_c\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstyle_mixing_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m             \u001b[0mpl_noise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_img\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_img\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgen_img\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pl_grads'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv2d_gradfix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_weight_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/src/current-approach/new/utils/stylegan2.py\u001b[0m in \u001b[0;36mrun_G\u001b[0;34m(G, z, c, style_mixing_prob)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0mcutoff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mws\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mstyle_mixing_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcutoff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcutoff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mws\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0mws\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcutoff\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_w_avg_update\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcutoff\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynthesis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mws\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mws\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/src/current-approach/new/networks/stylegan2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, ws, **block_kwargs)\u001b[0m\n\u001b[1;32m    470\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_ws\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock_resolutions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_ws\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m             \u001b[0mblock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'b{res}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 472\u001b[0;31m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_ws\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mblock_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    473\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/src/current-approach/new/networks/stylegan2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, img, ws, force_fp32, fused_modconv, **layer_kwargs)\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0;31m# Main layers.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_channels\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfused_modconv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfused_modconv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mlayer_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marchitecture\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'resnet'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mskip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/src/current-approach/new/networks/stylegan2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, w, noise_mode, fused_modconv, gain)\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0mflip_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# slightly faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m         x = modulated_conv2d(x=x, weight=self.weight, styles=styles, noise=noise, up=self.up,\n\u001b[0;32m--> 300\u001b[0;31m             padding=self.padding, resample_filter=self.resample_filter, flip_weight=flip_weight, fused_modconv=fused_modconv)\n\u001b[0m\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0mact_gain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact_gain\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/src/current-approach/new/torch_utils/misc.py\u001b[0m in \u001b[0;36mdecorator\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m     \u001b[0mdecorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/src/current-approach/new/networks/stylegan2.py\u001b[0m in \u001b[0;36mmodulated_conv2d\u001b[0;34m(x, weight, styles, noise, up, down, padding, resample_filter, demodulate, flip_weight, fused_modconv)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdemodulate\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfused_modconv\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# [NOIkk]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mstyles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# [NOIkk]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdemodulate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mdcoefs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrsqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# [NO]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cannot reshape tensor of 0 elements into shape [0, 1, -1, 1, 1] because the unspecified dimension size -1 can be any value and is ambiguous"
     ]
    }
   ],
   "source": [
    "batch_idx = 0\n",
    "cur_nimg = 0\n",
    "tick_start_nimg = cur_nimg\n",
    "cur_stats_interval_kimgs = stats_interval_kimgs\n",
    "cur_model_save_interval_kimgs = model_save_interval_kimgs\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i, data, in enumerate(train_loader):\n",
    "\n",
    "        #########################################\n",
    "        # data loading\n",
    "        real_img = data[0].to(device)\n",
    "        visibility_mask = data[1].to(device)\n",
    "        face_eye_mask = data[2].to(device)\n",
    "        shading_mask = data[3].to(device)\n",
    "        grid = data[4].to(device)\n",
    "        gen_lmks = data[5].to(device)\n",
    "        real_lmks = data[6].to(device)\n",
    "        z = data[7].to(device)\n",
    "        c = torch.zeros(batch_size, c_dim).to(device)\n",
    "        \n",
    "        #########################################\n",
    "        # discriminator\n",
    "        D.zero_grad()\n",
    "\n",
    "        if((batch_idx+1) % d_reg_interval == 0):\n",
    "            r1_reg_loss.run(real_img, c)\n",
    "        else:\n",
    "            uv_map, ws = run_G(G, z, c, style_mixing_prob)\n",
    "            rasterized_img = F.grid_sample(uv_map, grid, align_corners=False)\n",
    "            gen_img = (visibility_mask * rasterized_img * shading_mask) + (1 - visibility_mask) * real_img\n",
    "            discrim_loss.run(real_img, c, gen_img, c)\n",
    "        optimizer_D.step()\n",
    "\n",
    "        \n",
    "        #########################################\n",
    "        # generator\n",
    "        G.zero_grad()\n",
    "\n",
    "        if((batch_idx+ 1) % g_reg_interval == 0):\n",
    "            g_reg_loss.run(z, c)\n",
    "        else:\n",
    "            uv_map, ws = run_G(G, z, c, style_mixing_prob)\n",
    "            rasterized_img = F.grid_sample(uv_map, grid, align_corners=False)\n",
    "            gen_img = (visibility_mask * rasterized_img * shading_mask) + (1 - visibility_mask) * real_img\n",
    "\n",
    "            # gen loss\n",
    "            gen_logits = run_D(D, gen_img, c, None)\n",
    "            gen_loss = torch.nn.functional.softplus(-gen_logits).mean()\n",
    "             \n",
    "            # id loss\n",
    "            id_loss = 0#face_id_loss(gen_img, real_img, gen_lmks, real_lmks)\n",
    "\n",
    "            # l2 loss\n",
    "            l2_loss = 1000 * mse_loss(real_img, gen_img)\n",
    "            total_loss = gen_loss + id_loss + l2_loss\n",
    "            total_loss.backward()\n",
    "            training_stats.report('/Loss/G/loss', total_loss)\n",
    "            training_stats.report('/Loss/G/l2_loss', l2_loss)\n",
    "            training_stats.report('/Loss/G/id_loss', id_loss)\n",
    "            training_stats.report('/Loss/G/gen_loss', gen_loss)\n",
    "\n",
    "        optimizer_G.step()\n",
    "\n",
    "        \n",
    "        # Update G_ema.\n",
    "        with torch.autograd.profiler.record_function('Gema'):\n",
    "            G_ema.requires_grad_(False)\n",
    "            G.requires_grad_(False)\n",
    "            ema_nimg = ema_kimg * 1000\n",
    "            ema_beta = 0.5 ** (batch_size / max(ema_nimg, 1e-8))\n",
    "            for p_ema, p in zip(G_ema.parameters(), G.parameters()):\n",
    "                p_ema.copy_(p.lerp(p_ema, ema_beta))\n",
    "            for b_ema, b in zip(G_ema.buffers(), G.buffers()):\n",
    "                b_ema.copy_(b)\n",
    "            G.requires_grad_(True)\n",
    "\n",
    "\n",
    "        # updates augmentation probability\n",
    "        if ((batch_idx + 1) % ada_interval == 0):\n",
    "            ada_stats.update()\n",
    "            adjust = np.sign(ada_stats['Loss/signs/real'] - ada_target) * (batch_size * ada_interval) / (ada_kimg * 1000)\n",
    "            augment_pipe.p.copy_((augment_pipe.p + adjust).max(misc.constant(0, device=device)))\n",
    "\n",
    "        # statistics update\n",
    "        if(cur_nimg > cur_stats_interval_kimgs * 1000):\n",
    "            cur_stats_interval_kimgs = cur_stats_interval_kimgs + stats_interval_kimgs\n",
    "            stats_collector.update()\n",
    "            stats_dict = stats_collector.as_dict()\n",
    "            timestamp = time.time()\n",
    "            if stats_jsonl is not None:\n",
    "                fields = dict(stats_dict, timestamp=timestamp)\n",
    "                stats_jsonl.write(json.dumps(fields) + '\\n')\n",
    "                stats_jsonl.flush()\n",
    "            torchvision.utils.save_image(torchvision.utils.make_grid(gen_img.detach().cpu()), os.path.join(run_dir, f'fakes.png'))\n",
    "            torchvision.utils.save_image(torchvision.utils.make_grid(real_img.detach().cpu()), os.path.join(run_dir, f'real.png'))\n",
    "            torchvision.utils.save_image(torchvision.utils.make_grid(uv_map.detach().cpu()), os.path.join(run_dir, f'uv.png'))\n",
    "\n",
    "        if(cur_nimg > cur_model_save_interval_kimgs * 1000):\n",
    "            cur_model_save_interval_kimgs = cur_model_save_interval_kimgs + model_save_interval_kimgs\n",
    "            # Save network snapshot.\n",
    "            snapshot_data = {}\n",
    "            for name, module in [('G', G), ('D', D), ('G_ema', G_ema), ('augment_pipe', augment_pipe)]:\n",
    "                if module is not None:\n",
    "                    module = copy.deepcopy(module).eval().requires_grad_(False).cpu()\n",
    "                snapshot_data[name] = module\n",
    "                del module # conserve memory\n",
    "            snapshot_pkl = os.path.join(run_dir, f'network-snapshot-{cur_nimg//1000:06d}.pkl')\n",
    "            with open(snapshot_pkl, 'wb') as f:\n",
    "                pickle.dump(snapshot_data, f)\n",
    "            del snapshot_data\n",
    "            torchvision.utils.save_image(torchvision.utils.make_grid(gen_img.detach().cpu()), os.path.join(run_dir, f'{cur_nimg//1000:06d}fakes.png'))\n",
    "            torchvision.utils.save_image(torchvision.utils.make_grid(real_img.detach().cpu()), os.path.join(run_dir, f'{cur_nimg//1000:06d}real.png'))\n",
    "            torchvision.utils.save_image(torchvision.utils.make_grid(uv_map.detach().cpu()), os.path.join(run_dir, f'{cur_nimg//1000:06d}uv.png'))\n",
    "\n",
    "        # Update state.\n",
    "        cur_nimg += batch_size\n",
    "        batch_idx += 1\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cfg_dict = {\n",
    "  \"DECA_kwargs\": {\n",
    "    \"pretrained_model\" : '/src/deca/DECA/data/deca_model.tar',\n",
    "    \"topology_path\": '/src/deca/DECA/data/head_template.obj',\n",
    "    'dense_template_path': '/src/deca/DECA/data/texture_data_256.npy',\n",
    "    'fixed_displacement_path': '/src/deca/DECA/data/fixed_displacement_256.npy',\n",
    "    'flame_model_path': '/src/deca/DECA/data/generic_model.pkl',\n",
    "    'flame_lmk_embedding_path': '/src/deca/DECA/data/landmark_embedding.npy',\n",
    "    'face_mask_path': '/src/deca/DECA/data/uv_face_mask.png',\n",
    "    'face_eye_mask_path': '/src/deca/DECA/data/uv_face_eye_mask.png',\n",
    "    'mean_tex_path': '/src/deca/DECA/data/mean_texture.jpg',\n",
    "    'tex_path': '/src/deca/DECA/data/FLAME_albedo_from_BFM.npz',\n",
    "    'tex_type': 'BFM',\n",
    "    'image_size': 224,\n",
    "    'uv_size': 256,\n",
    "    'param_list': ['shape', 'tex', 'exp', 'pose', 'cam', 'light'],\n",
    "    'n_shape': 100,\n",
    "    'n_tex': 50,\n",
    "    'n_exp': 50,\n",
    "    'n_cam': 3,\n",
    "    'n_pose': 6,\n",
    "    'n_light': 27,\n",
    "    'use_tex': True, \n",
    "    'jaw_type': 'aa',\n",
    "    'fr_model_path': '/src/deca/DECA/data/resnet50_ft_weight.pkl', \n",
    "    'n_detail': 128, \n",
    "    'max_z': 0.01,\n",
    "    'jaw_type' : 'euler'\n",
    "  },\n",
    "}\n",
    "\n",
    "cfg = DefaultMunch.fromDict(cfg_dict)\n",
    "\n",
    "\n",
    "from networks.FLAME import FLAME, FLAMETex\n",
    "\n",
    "flametex = FLAMETex(cfg.DECA_kwargs).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'G_ema' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1322724/1519617248.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mG_ema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'G_ema' is not defined"
     ]
    }
   ],
   "source": [
    "del G_ema\n",
    "del G\n",
    "del D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 2304.00 GiB (GPU 0; 20.00 GiB total capacity; 153.00 MiB already allocated; 16.58 GiB free; 170.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1322724/1210891204.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mbt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflametex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexture_basis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0ma_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflametex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexture_mean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprecision_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mresized_uv_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muv_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muv_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0malbedo_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprecision_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ma_mean\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mresized_uv_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 2304.00 GiB (GPU 0; 20.00 GiB total capacity; 153.00 MiB already allocated; 16.58 GiB free; 170.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "b = flametex.texture_basis.squeeze(0).T\n",
    "bt = flametex.texture_basis.squeeze(0)\n",
    "a_mean = flametex.texture_mean.squeeze(0)\n",
    "precision_matrix = torch.inverse(torch.matmul(bt, b)) \n",
    "resized_uv_map = F.interpolate(uv_map, [512, 512]).reshape(len(uv_map), -1)\n",
    "albedo_loss = torch.linalg.norm(torch.matmul(torch.matmul(precision_matrix, bt), (a_mean - resized_uv_map).T))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = flametex.texture_basis.squeeze(0).T\n",
    "bt = flametex.texture_basis.squeeze(0)\n",
    "a_mean = flametex.texture_mean.squeeze(0)\n",
    "precision_matrix = torch.inverse(torch.matmul(bt, b)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([32.2076], device='cuda:0', grad_fn=<CopyBackwards>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "albedo_loss = torch.linalg.norm(torch.matmul(torch.matmul(precision_matrix, bt), (a_mean - resized_uv_map).T), axis=0).mean()\n",
    "albedo_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 50])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0428, -0.0329, -0.0563, -0.0578],\n",
       "        [-0.0428, -0.0329, -0.0563, -0.0578],\n",
       "        [-0.0575, -0.0428, -0.0730, -0.0794],\n",
       "        ...,\n",
       "        [ 0.0207,  0.0179,  0.0202,  0.0101],\n",
       "        [ 0.0003, -0.0039, -0.0042, -0.0099],\n",
       "        [ 0.0003, -0.0039, -0.0042, -0.0099]], device='cuda:0',\n",
       "       grad_fn=<PermuteBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resized_uv_map.T"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
